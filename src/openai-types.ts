/**
 * Represents the request body for creating a chat completion.
 */
export interface ChatCompletionRequest {
  /** A list of messages comprising the conversation so far. */
  messages: Message[];

  /** ID of the model to use. Must be one that the provider supports. */
  model: string;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
   * @default 0
   */
  frequency_penalty?: number;

  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
   */
  logit_bias?: Record<string, number> | null;

  /**
   * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
   * @default false
   */
  logprobs?: boolean;

  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
   */
  top_logprobs?: number;

  /**
   * The maximum number of tokens that can be generated in the chat completion.
   * The total length of input tokens and generated tokens is limited by the model's context length.
   */
  max_tokens?: number;

  /**
   * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
   * @default 1
   */
  n?: number;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   * @default 0
   */
  presence_penalty?: number;

  /**
   * An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
   * Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
   */
  response_format?: {
    /** Must be one of `text` or `json_object`. */
    type: "text" | "json_object";
  };

  /**
   * If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
   * Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
   */
  seed?: number;

  /**
   * Up to 4 sequences where the API will stop generating further tokens.
   */
  stop?: string | string[] | null;

  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.
   * @default false
   */
  stream?: boolean;

  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
   * We generally recommend altering this or `top_p` but not both.
   * @default 1
   */
  temperature?: number;

  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
   * We generally recommend altering this or `temperature` but not both.
   * @default 1
   */
  top_p?: number;

  /** A list of tools the model may call. Currently, only functions are supported as a tool. */
  tools?: Tool[];

  /**
   * Controls which (if any) function is called by the model.
   * `none` means the model will not call a function and instead generates a message.
   * `auto` means the model can pick between generating a message or calling a function.
   * Specifying a particular function via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that function.
   * `none` is the default when no functions are present. `auto` is the default if functions are present.
   */
  tool_choice?:
    | "none"
    | "auto"
    | { type: "function"; function: { name: string } };

  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
   */
  user?: string;

  // Deprecated properties
  /** @deprecated Use `tool_choice` instead. */
  function_call?: "none" | "auto" | { name: string };

  /** @deprecated Use `tools` instead. */
  functions?: Function[];
}

/**
 * Represents a message in the conversation.
 */
export type Message =
  | SystemMessage
  | UserMessage
  | AssistantMessage
  | ToolMessage
  | FunctionMessage;

export interface SystemMessage {
  /** The contents of the system message. */
  content: string;
  /** The role of the message's author, in this case `system`. */
  role: "system";
  /** An optional name for the participant. */
  name?: string;
}

export interface UserMessage {
  /** The contents of the user message. */
  content: string | ContentPart[];
  /** The role of the message's author, in this case `user`. */
  role: "user";
  /** An optional name for the participant. */
  name?: string;
}

export interface AssistantMessage {
  /** The contents of the assistant message. */
  content: string | null;
  /** The role of the message's author, in this case `assistant`. */
  role: "assistant";
  /** An optional name for the participant. */
  name?: string;
  /** The tool calls generated by the model, such as function calls. */
  tool_calls?: ToolCall[];
  /** @deprecated Use `tool_calls` instead. */
  function_call?: FunctionCall;
}

export interface ToolMessage {
  /** The contents of the tool message. */
  content: string;
  /** The role of the message's author, in this case `tool`. */
  role: "tool";
  /** Tool call that this message is responding to. */
  tool_call_id: string;
}

/** @deprecated Use ToolMessage instead. */
export interface FunctionMessage {
  /** The contents of the function message. */
  content: string | null;
  /** The role of the message's author, in this case `function`. */
  role: "function";
  /** The name of the function to call. */
  name: string;
}

export interface ContentPart {
  type: "text" | "image_url";
  text?: string;
  image_url?: {
    url: string;
    detail: "auto" | "low" | "high";
  };
}

export interface Tool {
  /** The type of the tool. Currently, only `function` is supported. */
  type: "function";
  /** The function that can be called. */
  function: Function;
}

export interface Function {
  /** A description of what the function does. */
  description?: string;
  /** The name of the function to be called. */
  name: string;
  /**
   * The parameters the function accepts, described as a JSON Schema object.
   * Omitting `parameters` defines a function with an empty parameter list.
   */
  parameters?: Record<string, any>;
}

export interface ToolCall {
  /** The ID of the tool call. */
  id: string;
  /** The type of the tool. Currently, only `function` is supported. */
  type: "function";
  /** The function that the model called. */
  function: FunctionCall;
}

export interface FunctionCall {
  /** The name of the function to call. */
  name: string;
  /**
   * The arguments to call the function with, as generated by the model in JSON format.
   * Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema.
   * Validate the arguments in your code before calling your function.
   */
  arguments: string;
}

/**
 * Represents a chat completion response returned by the model.
 */
export interface ChatCompletionResponse {
  /** A unique identifier for the chat completion. */
  id: string;
  /** A list of chat completion choices. Can be more than one if `n` is greater than 1. */
  choices: ChatCompletionChoice[];
  /** The Unix timestamp (in seconds) of when the chat completion was created. */
  created: number;
  /** The model used for the chat completion. */
  model: string;
  /**
   * This fingerprint represents the backend configuration that the model runs with.
   * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
   */
  system_fingerprint: string;
  /** The object type, which is always `chat.completion`. */
  object: "chat.completion";
  /** Usage statistics for the completion request. */
  usage: Usage;
}

export interface ChatCompletionChoice {
  /**
   * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
   * `length` if the maximum number of tokens specified in the request was reached,
   * `content_filter` if content was omitted due to a flag from our content filters,
   * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
   */
  finish_reason:
    | "stop"
    | "length"
    | "tool_calls"
    | "content_filter"
    | "function_call";
  /** The index of the choice in the list of choices. */
  index: number;
  /** A chat completion message generated by the model. */
  message: AssistantMessage;
  /** Log probability information for the choice. */
  logprobs: LogProbs | null;
}

export interface LogProbs {
  /** A list of message content tokens with log probability information. */
  content: TokenLogProb[] | null;
}

export interface TokenLogProb {
  /** The token. */
  token: string;
  /**
   * The log probability of this token, if it is within the top 20 most likely tokens.
   * Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
   */
  logprob: number;
  /**
   * A list of integers representing the UTF-8 bytes representation of the token.
   * Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation.
   * Can be `null` if there is no bytes representation for the token.
   */
  bytes: number[] | null;
  /** List of the most likely tokens and their log probability, at this token position. */
  top_logprobs: TopLogProb[];
}

export interface TopLogProb {
  /** The token. */
  token: string;
  /**
   * The log probability of this token, if it is within the top 20 most likely tokens.
   * Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
   */
  logprob: number;
  /**
   * A list of integers representing the UTF-8 bytes representation of the token.
   * Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation.
   * Can be `null` if there is no bytes representation for the token.
   */
  bytes: number[] | null;
}

export interface Usage {
  /** Number of tokens in the generated completion. */
  completion_tokens: number;
  /** Number of tokens in the prompt. */
  prompt_tokens: number;
  /** Total number of tokens used in the request (prompt + completion). */
  total_tokens: number;
}

/**
 * Represents a chat completion chunk in a streamed response.
 */
export interface ChatCompletionChunk {
  id: string;
  object: "chat.completion.chunk";
  created: number;
  model: string;
  system_fingerprint: string | null;
  service_tier?: string;
  usage?: Usage;
  choices: ChatCompletionChunkChoice[];
  x_groq?: Record<string, any>;
  x_actionschema?: Record<string, any>;
}

export interface ChatCompletionChunkChoice {
  index: number;
  delta: {
    role?: string;
    content?: string;
    tool_calls?: ToolCall[];
    tools?: Tool[];
  };
  logprobs?: Record<string, any> | null;
  finish_reason?: string | null;
}
